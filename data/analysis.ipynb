{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511581aa",
   "metadata": {},
   "source": [
    "### Natural break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b24cf132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OPTIMAL BREAK POINT ANALYSIS (n_classes=6)\n",
      "================================================================================\n",
      "\n",
      "Jenks Natural Breaks:\n",
      "  Breaks: ['113', '117', '120', '134', '135', '150']\n",
      "  Distribution:\n",
      "    Class 1: 10115 (100.0%)\n",
      "    Class 2:     1 (  0.0%)\n",
      "    Class 3:     1 (  0.0%)\n",
      "    Class 4:     1 (  0.0%)\n",
      "    Class 5:     1 (  0.0%)\n",
      "    Class 6:     1 (  0.0%)\n",
      "\n",
      "K-Means Clustering:\n",
      "  Breaks: ['27', '37', '46', '55', '76', '150']\n",
      "  Distribution:\n",
      "    Class 1:  1281 ( 12.7%)\n",
      "    Class 2:  2316 ( 22.9%)\n",
      "    Class 3:  2735 ( 27.0%)\n",
      "    Class 4:  2470 ( 24.4%)\n",
      "    Class 5:  1234 ( 12.2%)\n",
      "    Class 6:    84 (  0.8%)\n",
      "\n",
      "Quantile (Equal Count):\n",
      "  Breaks: ['30', '36', '42', '47', '53', '150']\n",
      "  Distribution:\n",
      "    Class 1:  1688 ( 16.7%)\n",
      "    Class 2:  1685 ( 16.7%)\n",
      "    Class 3:  1690 ( 16.7%)\n",
      "    Class 4:  1683 ( 16.6%)\n",
      "    Class 5:  1688 ( 16.7%)\n",
      "    Class 6:  1686 ( 16.7%)\n",
      "\n",
      "Equal Interval:\n",
      "  Breaks: ['34', '57', '80', '103', '127', '150']\n",
      "  Distribution:\n",
      "    Class 1:  2710 ( 26.8%)\n",
      "    Class 2:  6445 ( 63.7%)\n",
      "    Class 3:   903 (  8.9%)\n",
      "    Class 4:    47 (  0.5%)\n",
      "    Class 5:    12 (  0.1%)\n",
      "    Class 6:     3 (  0.0%)\n",
      "\n",
      "Standard Deviation:\n",
      "  Breaks: ['17', '29', '42', '54', '67', '79']\n",
      "  Distribution:\n",
      "    Class 1:   150 (  1.5%)\n",
      "    Class 2:  1466 ( 14.5%)\n",
      "    Class 3:  3510 ( 34.7%)\n",
      "    Class 4:  3534 ( 34.9%)\n",
      "    Class 5:  1258 ( 12.4%)\n",
      "    Class 6:   134 (  1.3%)\n",
      "\n",
      "================================================================================\n",
      "GOODNESS OF VARIANCE FIT (GVF) - Higher is Better\n",
      "================================================================================\n",
      "Jenks Natural Breaks.............................. GVF = 0.0257\n",
      "K-Means Clustering................................ GVF = 0.9284\n",
      "Quantile (Equal Count)............................ GVF = 0.8808\n",
      "Equal Interval.................................... GVF = 0.7734\n",
      "Standard Deviation................................ GVF = 0.9251\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/api/APIMS-monthly.csv')\n",
    "\n",
    "# 1. JENKS NATURAL BREAKS (Fisher-Jenks algorithm)\n",
    "# Best for finding natural groupings in data\n",
    "def jenks_breaks(data, n_classes):\n",
    "    \"\"\"\n",
    "    Compute Jenks Natural Breaks classification\n",
    "    \"\"\"\n",
    "    data = np.array(sorted(data))\n",
    "    n = len(data)\n",
    "    \n",
    "    # Initialize matrices\n",
    "    mat1 = np.zeros((n, n_classes))\n",
    "    mat2 = np.zeros((n, n_classes))\n",
    "    \n",
    "    for i in range(n):\n",
    "        mat1[i, 0] = 1\n",
    "        mat2[i, 0] = 0\n",
    "        for j in range(1, n_classes):\n",
    "            mat1[i, j] = float('inf')\n",
    "    \n",
    "    # Compute variance for each subset\n",
    "    for l in range(2, n + 1):\n",
    "        s1 = s2 = w = 0\n",
    "        for m in range(1, l + 1):\n",
    "            i3 = l - m + 1\n",
    "            val = data[i3 - 1]\n",
    "            \n",
    "            s2 += val * val\n",
    "            s1 += val\n",
    "            w += 1\n",
    "            \n",
    "            v = s2 - (s1 * s1) / w\n",
    "            i4 = i3 - 1\n",
    "            \n",
    "            if i4 != 0:\n",
    "                for j in range(2, n_classes + 1):\n",
    "                    if mat1[l - 1, j - 1] >= v + mat1[i4 - 1, j - 2]:\n",
    "                        mat1[l - 1, j - 1] = v + mat1[i4 - 1, j - 2]\n",
    "                        mat2[l - 1, j - 1] = i3\n",
    "    \n",
    "    # Extract breaks\n",
    "    k = n\n",
    "    kclass = [0] * (n_classes + 1)\n",
    "    kclass[n_classes] = data[-1]\n",
    "    \n",
    "    for j in range(n_classes, 1, -1):\n",
    "        id_val = int(mat2[k - 1, j - 1] - 2)\n",
    "        kclass[j - 1] = data[id_val]\n",
    "        k = int(mat2[k - 1, j - 1] - 1)\n",
    "    \n",
    "    return kclass[1:]\n",
    "\n",
    "# 2. K-MEANS CLUSTERING\n",
    "def kmeans_breaks(data, n_classes):\n",
    "    \"\"\"\n",
    "    Use K-means clustering to find breaks\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_classes, random_state=42)\n",
    "    data_reshaped = data.values.reshape(-1, 1)\n",
    "    kmeans.fit(data_reshaped)\n",
    "    \n",
    "    # Get cluster centers and sort them\n",
    "    centers = sorted(kmeans.cluster_centers_.flatten())\n",
    "    \n",
    "    # Breaks are midpoints between centers\n",
    "    breaks = [(centers[i] + centers[i+1]) / 2 for i in range(len(centers)-1)]\n",
    "    breaks.append(data.max())\n",
    "    \n",
    "    return breaks\n",
    "\n",
    "# 3. QUANTILE-BASED (Equal Count)\n",
    "def quantile_breaks(data, n_classes):\n",
    "    \"\"\"\n",
    "    Equal count breaks using quantiles\n",
    "    \"\"\"\n",
    "    quantiles = np.linspace(0, 1, n_classes + 1)[1:]\n",
    "    breaks = [data.quantile(q) for q in quantiles]\n",
    "    return breaks\n",
    "\n",
    "# 4. EQUAL INTERVAL\n",
    "def equal_interval_breaks(data, n_classes):\n",
    "    \"\"\"\n",
    "    Equal interval breaks\n",
    "    \"\"\"\n",
    "    min_val = data.min()\n",
    "    max_val = data.max()\n",
    "    interval = (max_val - min_val) / n_classes\n",
    "    breaks = [min_val + interval * (i + 1) for i in range(n_classes)]\n",
    "    return breaks\n",
    "\n",
    "# 5. STANDARD DEVIATION\n",
    "def std_breaks(data, n_classes):\n",
    "    \"\"\"\n",
    "    Standard deviation breaks\n",
    "    \"\"\"\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    \n",
    "    # Create breaks at mean Â± std intervals\n",
    "    breaks = []\n",
    "    for i in range(-(n_classes // 2), (n_classes // 2) + 1):\n",
    "        if i != -(n_classes // 2):  # Skip first break\n",
    "            breaks.append(mean + i * std)\n",
    "    \n",
    "    # Adjust to data range\n",
    "    breaks = [max(data.min(), min(data.max(), b)) for b in breaks]\n",
    "    return sorted(set(breaks))\n",
    "\n",
    "# Compare all methods\n",
    "n_classes = 6\n",
    "adt_data = df['aqi']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"OPTIMAL BREAK POINT ANALYSIS (n_classes={n_classes})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "methods = {\n",
    "    'Jenks Natural Breaks': jenks_breaks(adt_data, n_classes),\n",
    "    'K-Means Clustering': kmeans_breaks(adt_data, n_classes),\n",
    "    'Quantile (Equal Count)': quantile_breaks(adt_data, n_classes),\n",
    "    'Equal Interval': equal_interval_breaks(adt_data, n_classes),\n",
    "    'Standard Deviation': std_breaks(adt_data, n_classes)\n",
    "}\n",
    "\n",
    "for method_name, breaks in methods.items():\n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  Breaks: {[f'{b:,.0f}' for b in breaks]}\")\n",
    "    \n",
    "    # Calculate how many observations in each class\n",
    "    bins = [adt_data.min() - 1] + breaks\n",
    "    labels = [f'Class {i+1}' for i in range(len(breaks))]\n",
    "    adt_data_classified = pd.cut(adt_data, bins=bins, labels=labels, include_lowest=True)\n",
    "    counts = adt_data_classified.value_counts().sort_index()\n",
    "    \n",
    "    print(f\"  Distribution:\")\n",
    "    for label, count in counts.items():\n",
    "        pct = (count / len(adt_data)) * 100\n",
    "        print(f\"    {label}: {count:>5} ({pct:>5.1f}%)\")\n",
    "\n",
    "# 6. GOODNESS OF VARIANCE FIT (GVF)\n",
    "# Measures how well the classification preserves variance\n",
    "def calculate_gvf(data, breaks):\n",
    "    \"\"\"\n",
    "    Calculate Goodness of Variance Fit\n",
    "    Higher is better (closer to 1.0)\n",
    "    \"\"\"\n",
    "    bins = [data.min() - 1] + breaks\n",
    "    classes = pd.cut(data, bins=bins, include_lowest=True)\n",
    "    \n",
    "    # Total sum of squared deviations\n",
    "    sdam = ((data - data.mean()) ** 2).sum()\n",
    "    \n",
    "    # Within-class sum of squared deviations\n",
    "    sdcm = 0\n",
    "    for class_label in classes.unique():\n",
    "        if pd.notna(class_label):\n",
    "            class_data = data[classes == class_label]\n",
    "            sdcm += ((class_data - class_data.mean()) ** 2).sum()\n",
    "    \n",
    "    gvf = (sdam - sdcm) / sdam\n",
    "    return gvf\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GOODNESS OF VARIANCE FIT (GVF) - Higher is Better\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for method_name, breaks in methods.items():\n",
    "    gvf = calculate_gvf(adt_data, breaks)\n",
    "    print(f\"{method_name:.<50} GVF = {gvf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f72cc",
   "metadata": {},
   "source": [
    "### Top k statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0bcd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the cleaned data\n",
    "melted_df = pd.read_csv('../data/traffic/adt_data.csv')\n",
    "\n",
    "# find top k busiest roads by year\n",
    "k = 25\n",
    "top_k_by_year = melted_df.groupby('Year').apply(lambda x: x.nlargest(k, 'ADT')).reset_index(drop=True)\n",
    "\n",
    "top_k_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all unique stations in the top k list\n",
    "unique_stations = top_k_by_year['Location'].unique()\n",
    "unique_stations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
